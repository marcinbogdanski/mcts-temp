{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is proof-of-concept bare-bones implementation of **AlphaZero** algorithm. In this notebook algorithm is tested on **TicTacToe** board game. \n",
    "\n",
    "Outside of this notebook it was tested on more complex **Othello** game, where after about one week of training it started to achieve reasonable performance even with some \"human like\" behaviours.\n",
    "\n",
    "Main components of the algorithm are:\n",
    "* _Monte-Carlo Tree Search_ used for look ahead\n",
    "* _Convolutional Neural Network_ with Policy and Value heads\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**NOTE #1:** this code works, but is **not tidied or optimised in any way**. All debug, logging, assert, testing etc. code was stripped down when copying to this notebook.\n",
    "\n",
    "When I have some time I plan to cleanup this code, provide appropriate description and include Othello training code.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to run this notebook end-to-end and play against trained agent at the end. Worked on mine :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dependencies so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Board():\n",
    "    \"\"\"Represents single game board state, pieces, currently available moves etc.\n",
    "    \n",
    "    Attributes:\n",
    "        n (int): size of the board, default: 3\n",
    "        pieces (2-nested-list-of-int): board state, with following properties:\n",
    "            allowed values are: -1 is black (X), +1 is white (O), 0 is empty\n",
    "            e.g. 3-size empty board would be: [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n=3):\n",
    "        \"Set up initial board configuration.\"\n",
    "\n",
    "        self.n = n\n",
    "        # Create the empty board array.\n",
    "        self.pieces = [None]*self.n\n",
    "        for i in range(self.n):\n",
    "            self.pieces[i] = [0]*self.n\n",
    "\n",
    "    # add [][] indexer syntax to the Board\n",
    "    def __getitem__(self, index): \n",
    "        return self.pieces[index]\n",
    "    \n",
    "    def get_legal_moves(self, color):\n",
    "        \"\"\"Returns all the legal moves for the given color.\n",
    "        \n",
    "        Params:\n",
    "            color: unused\n",
    "            \n",
    "        Returns:\n",
    "            list-of-tuple-of-int: all legal moves in form: [(0, 0), ...]\n",
    "        \"\"\"\n",
    "        moves = set()  # stores the legal moves.\n",
    "\n",
    "        # Get all the empty squares (color==0)\n",
    "        for y in range(self.n):\n",
    "            for x in range(self.n):\n",
    "                if self[x][y]==0:\n",
    "                    newmove = (x,y)\n",
    "                    moves.add(newmove)\n",
    "        return list(moves)\n",
    "    \n",
    "    def has_legal_moves(self):\n",
    "        for y in range(self.n):\n",
    "            for x in range(self.n):\n",
    "                if self[x][y]==0:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def is_win(self, color):\n",
    "        \"\"\"Check whether the given player has collected a triplet in any direction; \n",
    "        \n",
    "        Params:\n",
    "            color (int): -1 for black, 1 for white\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if 'color' won the game, False if game ongoing, draw or loss\n",
    "        \"\"\"\n",
    "        win = self.n\n",
    "        # check y-strips\n",
    "        for y in range(self.n):\n",
    "            count = 0\n",
    "            for x in range(self.n):\n",
    "                if self[x][y]==color:\n",
    "                    count += 1\n",
    "            if count==win:\n",
    "                return True\n",
    "        # check x-strips\n",
    "        for x in range(self.n):\n",
    "            count = 0\n",
    "            for y in range(self.n):\n",
    "                if self[x][y]==color:\n",
    "                    count += 1\n",
    "            if count==win:\n",
    "                return True\n",
    "        # check two diagonal strips\n",
    "        count = 0\n",
    "        for d in range(self.n):\n",
    "            if self[d][d]==color:\n",
    "                count += 1\n",
    "        if count==win:\n",
    "            return True\n",
    "        count = 0\n",
    "        for d in range(self.n):\n",
    "            if self[d][self.n-d-1]==color:\n",
    "                count += 1\n",
    "        if count==win:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def execute_move(self, move, color):\n",
    "        \"\"\"Perform the given move on the board; \n",
    "        color gives the color pf the piece to play (1=white,-1=black)\n",
    "        \"\"\"\n",
    "\n",
    "        (x,y) = move\n",
    "\n",
    "        # Add the piece to the empty square.\n",
    "        assert self[x][y] == 0\n",
    "        self[x][y] = color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(board.pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (0, 0), (0, 2), (2, 1), (2, 0), (2, 2), (1, 0), (1, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.get_legal_moves(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.execute_move((1, 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, -1, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally uses Board class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Game class implementation for the game of TicTacToe.\n",
    "Based on the OthelloGame then getGameEnded() was adapted to new rules.\n",
    "\"\"\"\n",
    "class TicTacToeGame:\n",
    "    \"\"\"\n",
    "    This class specifies the base Game class. To define your own game, subclass\n",
    "    this class and implement the functions below. This works when the game is\n",
    "    two-player, adversarial and turn-based.\n",
    "\n",
    "    Use 1 for player1 and -1 for player2.\n",
    "\n",
    "    See othello/OthelloGame.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "\n",
    "    def getInitBoard(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            startBoard: a representation of the board (ideally this is the form\n",
    "                        that will be the input to your neural network)\n",
    "        \"\"\"\n",
    "        # return initial board (numpy board)\n",
    "        b = Board(self.n)\n",
    "        return np.array(b.pieces)\n",
    "\n",
    "    def getBoardSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (x,y): a tuple of board dimensions\n",
    "        \"\"\"\n",
    "        # (a,b) tuple\n",
    "        return (self.n, self.n)\n",
    "\n",
    "    def getActionSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            actionSize: number of all possible actions\n",
    "        \"\"\"\n",
    "        # return number of actions\n",
    "        return self.n*self.n + 1\n",
    "\n",
    "    def getNextState(self, board, player, action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "            action: action taken by current player\n",
    "\n",
    "        Returns:\n",
    "            nextBoard: board after applying action\n",
    "            nextPlayer: player who plays in the next turn (should be -player)\n",
    "        \"\"\"\n",
    "        # if player takes action on board, return next (board,player)\n",
    "        # action must be a valid move\n",
    "        if action == self.n*self.n:\n",
    "            return (board, -player)\n",
    "        b = Board(self.n)\n",
    "        b.pieces = np.copy(board)\n",
    "        move = (int(action/self.n), action%self.n)\n",
    "        b.execute_move(move, player)\n",
    "        return (b.pieces, -player)\n",
    "\n",
    "    def getValidMoves(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player\n",
    "\n",
    "        Returns:\n",
    "            validMoves: a binary vector of length self.getActionSize(), 1 for\n",
    "                        moves that are valid from the current board and player,\n",
    "                        0 for invalid moves\n",
    "        \"\"\"\n",
    "        # return a fixed size binary vector\n",
    "        valids = [0]*self.getActionSize()\n",
    "        b = Board(self.n)\n",
    "        b.pieces = np.copy(board)\n",
    "        legalMoves =  b.get_legal_moves(player)\n",
    "        if len(legalMoves)==0:\n",
    "            valids[-1]=1\n",
    "            return np.array(valids)\n",
    "        for x, y in legalMoves:\n",
    "            valids[self.n*x+y]=1\n",
    "        return np.array(valids)\n",
    "\n",
    "    def getGameEnded(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "\n",
    "        Returns:\n",
    "            r: 0 if game has not ended. 1 if player won, -1 if player lost,\n",
    "               small non-zero value for draw.\n",
    "               \n",
    "        \"\"\"\n",
    "        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n",
    "        # player = 1\n",
    "        b = Board(self.n)\n",
    "        b.pieces = np.copy(board)\n",
    "\n",
    "        if b.is_win(player):\n",
    "            return 1\n",
    "        if b.is_win(-player):\n",
    "            return -1\n",
    "        if b.has_legal_moves():\n",
    "            return 0\n",
    "        # draw has a very little value \n",
    "        return 1e-4\n",
    "\n",
    "    def getCanonicalForm(self, board, player):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            player: current player (1 or -1)\n",
    "\n",
    "        Returns:\n",
    "            canonicalBoard: returns canonical form of board. The canonical form\n",
    "                            should be independent of player. For e.g. in chess,\n",
    "                            the canonical form can be chosen to be from the pov\n",
    "                            of white. When the player is white, we can return\n",
    "                            board as is. When the player is black, we can invert\n",
    "                            the colors and return the board.\n",
    "        \"\"\"\n",
    "        # return state if player==1, else return -state if player==-1\n",
    "        return player*board\n",
    "\n",
    "    def getSymmetries(self, board, pi):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "            pi: policy vector of size self.getActionSize()\n",
    "\n",
    "        Returns:\n",
    "            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n",
    "                       form of the board and the corresponding pi vector. This\n",
    "                       is used when training the neural network from examples.\n",
    "        \"\"\"\n",
    "        # mirror, rotational\n",
    "        assert(len(pi) == self.n**2+1)  # 1 for pass\n",
    "        pi_board = np.reshape(pi[:-1], (self.n, self.n))\n",
    "        l = []\n",
    "\n",
    "        for i in range(1, 5):\n",
    "            for j in [True, False]:\n",
    "                newB = np.rot90(board, i)\n",
    "                newPi = np.rot90(pi_board, i)\n",
    "                if j:\n",
    "                    newB = np.fliplr(newB)\n",
    "                    newPi = np.fliplr(newPi)\n",
    "                l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n",
    "        return l\n",
    "\n",
    "    def stringRepresentation(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board\n",
    "\n",
    "        Returns:\n",
    "            boardString: a quick conversion of board to a string format.\n",
    "                         Required by MCTS for hashing.\n",
    "        \"\"\"\n",
    "        # 8x8 numpy array (canonical board)\n",
    "        return board.tostring()  # np.tostring() returns raw memory view as string\n",
    "\n",
    "def display(board):\n",
    "    n = board.shape[0]\n",
    "\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(n):\n",
    "        print (y,\"\", end=\"\")\n",
    "    print(\"\")\n",
    "    print(\"  \", end=\"\")\n",
    "    for _ in range(n):\n",
    "        print (\"-\", end=\"-\")\n",
    "    print(\"--\")\n",
    "    for y in range(n):\n",
    "        print(y, \"|\",end=\"\")    # print the row #\n",
    "        for x in range(n):\n",
    "            piece = board[y][x]    # get the piece to print\n",
    "            if piece == -1: print(\"X \",end=\"\")\n",
    "            elif piece == 1: print(\"O \",end=\"\")\n",
    "            else:\n",
    "                if x==n:\n",
    "                    print(\"-\",end=\"\")\n",
    "                else:\n",
    "                    print(\"- \",end=\"\")\n",
    "        print(\"|\")\n",
    "\n",
    "    print(\"  \", end=\"\")\n",
    "    for _ in range(n):\n",
    "        print (\"-\", end=\"-\")\n",
    "    print(\"--\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.getBoardSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.getActionSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, -1, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 4\n",
    "n = 3\n",
    "(int(action/n), action%n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb [[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "ns -1\n"
     ]
    }
   ],
   "source": [
    "tmp = game.getInitBoard()\n",
    "nextBoard, nextState = game.getNextState(board=tmp, player=1, action=4)\n",
    "print('nb', nextBoard)\n",
    "print('ns', nextState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = game.getInitBoard()\n",
    "game.getValidMoves(board=tmp, player=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([[1, -1, 1],\n",
    "                [1, -1, 1],\n",
    "                [-1, 1, -1]])\n",
    "game.getGameEnded(board=tmp, player=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  1, -1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([[ 0,  0,  0],\n",
    "                [ 0, -1,  0],\n",
    "                [ 0, -1,  1]])\n",
    "game.getCanonicalForm(board=tmp, player=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "board\n",
      "[[ 1  0  0]\n",
      " [-1 -1  0]\n",
      " [ 0  0  0]]\n",
      "pi [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "---\n",
      "board\n",
      "[[ 0  0  1]\n",
      " [ 0 -1 -1]\n",
      " [ 0  0  0]]\n",
      "pi [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "---\n",
      "board\n",
      "[[ 0 -1  1]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "pi [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "---\n",
      "board\n",
      "[[ 1 -1  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "pi [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "---\n",
      "board\n",
      "[[ 0  0  0]\n",
      " [ 0 -1 -1]\n",
      " [ 0  0  1]]\n",
      "pi [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---\n",
      "board\n",
      "[[ 0  0  0]\n",
      " [-1 -1  0]\n",
      " [ 1  0  0]]\n",
      "pi [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "---\n",
      "board\n",
      "[[ 0  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 1 -1  0]]\n",
      "pi [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "---\n",
      "board\n",
      "[[ 0  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0 -1  1]]\n",
      "pi [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "tmp = np.array([[ 0,  0,  0],\n",
    "                [ 0, -1,  0],\n",
    "                [ 0, -1,  1]])\n",
    "pi = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "syms = game.getSymmetries(board=tmp, pi=pi)\n",
    "for b, p in syms:\n",
    "    print('board')\n",
    "    print(b)\n",
    "    print('pi', p)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([[ 0,  0,  0],\n",
    "                [ 0, -1,  0],\n",
    "                [ 0, -1,  1]])\n",
    "game.stringRepresentation(board=tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random and Human-ineracting players for the game of TicTacToe.\n",
    "\n",
    "Author: Evgeny Tyurin, github.com/evg-tyurin\n",
    "Date: Jan 5, 2018.\n",
    "\n",
    "Based on the OthelloPlayers by Surag Nair.\n",
    "\n",
    "\"\"\"\n",
    "class RandomPlayer():\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def play(self, board):\n",
    "        a = np.random.randint(self.game.getActionSize())\n",
    "        valids = self.game.getValidMoves(board, 1)\n",
    "        while valids[a]!=1:\n",
    "            a = np.random.randint(self.game.getActionSize())\n",
    "        return a\n",
    "\n",
    "\n",
    "class HumanTicTacToePlayer():\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "\n",
    "    def play(self, board):\n",
    "        # display(board)\n",
    "        valid = self.game.getValidMoves(board, 1)\n",
    "        for i in range(len(valid)):\n",
    "            if valid[i]:\n",
    "                print(int(i/self.game.n), int(i%self.game.n))\n",
    "        while True: \n",
    "            # Python 3.x\n",
    "            a = input()\n",
    "            # Python 2.x \n",
    "            # a = raw_input()\n",
    "\n",
    "            x,y = [int(x) for x in a.split(' ')]\n",
    "            a = self.game.n * x + y if x!= -1 else self.game.n ** 2\n",
    "            if valid[a]:\n",
    "                break\n",
    "            else:\n",
    "                print('Invalid')\n",
    "\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = RandomPlayer(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "tmp = game.getInitBoard()\n",
    "desired_action = rp.play(tmp)\n",
    "print(desired_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TicTacToeNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NeuralNet for the game of TicTacToe.\n",
    "\n",
    "Author: Evgeny Tyurin, github.com/evg-tyurin\n",
    "Date: Jan 5, 2018.\n",
    "\n",
    "Based on the OthelloNNet by SourKream and Surag Nair.\n",
    "\"\"\"\n",
    "class TicTacToeNNet():\n",
    "    def __init__(self, game, args):\n",
    "        # game params\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.args = args\n",
    "\n",
    "        # Neural Net\n",
    "        self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n",
    "\n",
    "        x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n",
    "        # input -> Conv2D -> BatchNorm -> Activation\n",
    "        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n",
    "        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n",
    "        h_conv4_flat = Flatten()(h_conv4)       \n",
    "        # flat -> Dense -> BatchNorm -> Activation -> Dropout\n",
    "        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n",
    "        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n",
    "        # in -> Dense -> Softmax\n",
    "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n",
    "        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n",
    "\n",
    "        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n",
    "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Game:\n",
    "#     def getBoardSize(self):\n",
    "#         return (3, 3)\n",
    "#     def getActionSize(self):\n",
    "#         return 3 * 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': False,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/keras113/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/keras113/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToeGame(n=3)\n",
    "nnet = TicTacToeNNet(game, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi [[0.10005815 0.10038645 0.09910443 0.10027572 0.10015361 0.09942819\n",
      "  0.10046444 0.10013936 0.10007542 0.09991422]]\n",
      "v [[-0.00236602]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[0, 0, 0],\n",
    "               [0, 0, 0],\n",
    "               [0, 0, 0]]])\n",
    "\n",
    "x = np.random.randn(1, 3, 3)\n",
    "\n",
    "pi, v = nnet.model.predict(x)\n",
    "print('pi', pi)\n",
    "print('v', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNETWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNetWrapper:\n",
    "    \"\"\"\n",
    "    This class specifies the base NeuralNet class. To define your own neural\n",
    "    network, subclass this class and implement the functions below. The neural\n",
    "    network does not consider the current player, and instead only deals with\n",
    "    the canonical form of the board.\n",
    "\n",
    "    See othello/NNet.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, args):\n",
    "        self.nnet = TicTacToeNNet(game, args)\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        This function trains the neural network with examples obtained from\n",
    "        self-play.\n",
    "\n",
    "        Input:\n",
    "            examples: a list of training examples, where each example is of form\n",
    "                      (board, pi, v). pi is the MCTS informed policy vector for\n",
    "                      the given board, and v is its value. The examples has\n",
    "                      board in its canonical form.\n",
    "        \"\"\"\n",
    "        # examples: list of examples, each example is of form (board, pi, v)\n",
    "        input_boards, target_pis, target_vs = list(zip(*examples))\n",
    "        input_boards = np.asarray(input_boards)\n",
    "        target_pis = np.asarray(target_pis)\n",
    "        target_vs = np.asarray(target_vs)\n",
    "        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n",
    "\n",
    "    def predict(self, board):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            board: current board in its canonical form.\n",
    "\n",
    "        Returns:\n",
    "            pi: a policy vector for the current board- a numpy array of length\n",
    "                game.getActionSize\n",
    "            v: a float in [-1,1] that gives the value of the current board\n",
    "        \"\"\"\n",
    "        # board: np array with board\n",
    "        \n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        # preparing input\n",
    "        board = board[np.newaxis, :, :]\n",
    "\n",
    "        # run\n",
    "        pi, v = self.nnet.model.predict(board)\n",
    "\n",
    "        #print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return pi[0], v[0]\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        \"\"\"\n",
    "        Saves the current neural network (with its parameters) in\n",
    "        folder/filename\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Checkpoint Directory exists! \")\n",
    "        self.nnet.model.save_weights(filepath)\n",
    "\n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        \"\"\"\n",
    "        Loads parameters of the neural network from folder/filename\n",
    "        \"\"\"\n",
    "        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise(\"No model in path '{}'\".format(filepath))\n",
    "        self.nnet.model.load_weights(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': False,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToeGame(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_wrap = NNetWrapper(game, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet_wrap.board_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet_wrap.action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "bb = game.getInitBoard()\n",
    "p, v = nnet_wrap.predict(bb)\n",
    "print(p)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp==0:\n",
    "            bestA = np.argmax(counts)\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propogated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propogated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s]!=0:\n",
    "            # terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                \n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arena():\n",
    "    \"\"\"\n",
    "    An Arena class where any 2 agents can be pit against each other.\n",
    "    \"\"\"\n",
    "    def __init__(self, player1, player2, game, display=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            player 1,2: two functions that takes board as input, return action\n",
    "            game: Game object\n",
    "            display: a function that takes board as input and prints it (e.g.\n",
    "                     display in othello/OthelloGame). Is necessary for verbose\n",
    "                     mode.\n",
    "\n",
    "        see othello/OthelloPlayers.py for an example. See pit.py for pitting\n",
    "        human players/other baselines with each other.\n",
    "        \"\"\"\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.game = game\n",
    "        self.display = display\n",
    "\n",
    "    def playGame(self, verbose=False):\n",
    "        \"\"\"\n",
    "        Executes one episode of a game.\n",
    "\n",
    "        Returns:\n",
    "            either\n",
    "                winner: player who won the game (1 if player1, -1 if player2)\n",
    "            or\n",
    "                draw result returned from the game that is neither 1, -1, nor 0.\n",
    "        \"\"\"\n",
    "        players = [self.player2, None, self.player1]\n",
    "        curPlayer = 1\n",
    "        board = self.game.getInitBoard()\n",
    "        it = 0\n",
    "        while self.game.getGameEnded(board, curPlayer)==0:\n",
    "            it+=1\n",
    "            if verbose:\n",
    "                assert(self.display)\n",
    "                print(\"Turn \", str(it), \"Player \", str(curPlayer))\n",
    "                self.display(board)\n",
    "            action = players[curPlayer+1](self.game.getCanonicalForm(board, curPlayer))\n",
    "\n",
    "            valids = self.game.getValidMoves(self.game.getCanonicalForm(board, curPlayer),1)\n",
    "\n",
    "            if valids[action]==0:\n",
    "                print(action)\n",
    "                assert valids[action] >0\n",
    "            board, curPlayer = self.game.getNextState(board, curPlayer, action)\n",
    "        if verbose:\n",
    "            assert(self.display)\n",
    "            print(\"Game over: Turn \", str(it), \"Result \", str(self.game.getGameEnded(board, 1)))\n",
    "            self.display(board)\n",
    "        return self.game.getGameEnded(board, 1)\n",
    "\n",
    "    def playGames(self, num, verbose=False):\n",
    "        \"\"\"\n",
    "        Plays num games in which player1 starts num/2 games and player2 starts\n",
    "        num/2 games.\n",
    "\n",
    "        Returns:\n",
    "            oneWon: games won by player1\n",
    "            twoWon: games won by player2\n",
    "            draws:  games won by nobody\n",
    "        \"\"\"\n",
    "        eps = 0\n",
    "        maxeps = int(num)\n",
    "\n",
    "        num = int(num/2)\n",
    "        oneWon = 0\n",
    "        twoWon = 0\n",
    "        draws = 0\n",
    "        for _ in range(num):\n",
    "            gameResult = self.playGame(verbose=verbose)\n",
    "            if gameResult==1:\n",
    "                oneWon+=1\n",
    "            elif gameResult==-1:\n",
    "                twoWon+=1\n",
    "            else:\n",
    "                draws+=1\n",
    "            # bookkeeping + plot progress\n",
    "            eps += 1\n",
    "            print(f'Game completed: ({eps+1}/{maxeps})')\n",
    "\n",
    "        self.player1, self.player2 = self.player2, self.player1\n",
    "        \n",
    "        for _ in range(num):\n",
    "            gameResult = self.playGame(verbose=verbose)\n",
    "            if gameResult==-1:\n",
    "                oneWon+=1                \n",
    "            elif gameResult==1:\n",
    "                twoWon+=1\n",
    "            else:\n",
    "                draws+=1\n",
    "            # bookkeeping + plot progress\n",
    "            eps += 1\n",
    "            print(f'Game completed: ({eps+1}/{maxeps})')\n",
    "            \n",
    "        # bar.finish()\n",
    "\n",
    "        return oneWon, twoWon, draws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, nnet, args, args_nnet):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.pnet = self.nnet.__class__(self.game, args_nnet)  # the competitor network\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.skipFirstSelfPlay = False # can be overriden in loadTrainExamples()\n",
    "\n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "\n",
    "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
    "        uses temp=0.\n",
    "\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, else -1.\n",
    "        \"\"\"\n",
    "        trainExamples = []\n",
    "        board = self.game.getInitBoard()\n",
    "        self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board,self.curPlayer)\n",
    "            temp = int(episodeStep < self.args.tempThreshold)\n",
    "\n",
    "            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)\n",
    "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "            for b,p in sym:\n",
    "                trainExamples.append([b, self.curPlayer, p, None])\n",
    "\n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
    "\n",
    "            r = self.game.getGameEnded(board, self.curPlayer)\n",
    "\n",
    "            if r!=0:\n",
    "                return [(x[0],x[2],r*((-1)**(x[1]!=self.curPlayer))) for x in trainExamples]\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximium length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(1, self.args.numIters+1):\n",
    "            # bookkeeping\n",
    "            print('------ITER ' + str(i) + '------')\n",
    "            # examples of the iteration\n",
    "            if not self.skipFirstSelfPlay or i>1:\n",
    "                iterationTrainExamples = collections.deque([], maxlen=self.args.maxlenOfQueue)\n",
    "    \n",
    "                for eps in range(self.args.numEps):\n",
    "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
    "                    iterationTrainExamples += self.executeEpisode()\n",
    "    \n",
    "                    # bookkeeping + plot progress\n",
    "                    print(f'Round completed {eps+1}/{self.args.numEps}')\n",
    "\n",
    "                # save the iteration examples to the history \n",
    "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "                \n",
    "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
    "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
    "                self.trainExamplesHistory.pop(0)\n",
    "            # backup history to a file\n",
    "            # NB! the examples were collected using the model from the previous iteration, so (i-1)  \n",
    "            self.saveTrainExamples(i-1)\n",
    "            \n",
    "            # shuffle examples before training\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            random.shuffle(trainExamples)\n",
    "\n",
    "            # training new network, keeping a copy of the old one\n",
    "            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            pmcts = MCTS(self.game, self.pnet, self.args)\n",
    "            \n",
    "            self.nnet.train(trainExamples)\n",
    "            nmcts = MCTS(self.game, self.nnet, self.args)\n",
    "\n",
    "            print('PITTING AGAINST PREVIOUS VERSION')\n",
    "            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n",
    "                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n",
    "            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n",
    "\n",
    "            print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))\n",
    "            if pwins+nwins == 0 or float(nwins)/(pwins+nwins) < self.args.updateThreshold:\n",
    "                print('REJECTING NEW MODEL')\n",
    "                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')\n",
    "            else:\n",
    "                print('ACCEPTING NEW MODEL')\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n",
    "                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')                \n",
    "\n",
    "    def getCheckpointFile(self, iteration):\n",
    "        return 'checkpoint_' + str(iteration) + '.pth.tar'\n",
    "\n",
    "    def saveTrainExamples(self, iteration):\n",
    "        folder = self.args.checkpoint\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        filename = os.path.join(folder, self.getCheckpointFile(iteration)+\".examples\")\n",
    "        with open(filename, \"wb+\") as f:\n",
    "            pickle.Pickler(f).dump(self.trainExamplesHistory)\n",
    "        f.closed\n",
    "\n",
    "    def loadTrainExamples(self):\n",
    "        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n",
    "        examplesFile = modelFile+\".examples\"\n",
    "        if not os.path.isfile(examplesFile):\n",
    "            print(examplesFile)\n",
    "            r = input(\"File with trainExamples not found. Continue? [y|n]\")\n",
    "            if r != \"y\":\n",
    "                sys.exit()\n",
    "        else:\n",
    "            print(\"File with trainExamples found. Read it.\")\n",
    "            with open(examplesFile, \"rb\") as f:\n",
    "                self.trainExamplesHistory = pickle.Unpickler(f).load()\n",
    "            f.closed\n",
    "            # examples based on the model were already collected (loaded)\n",
    "            self.skipFirstSelfPlay = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_net = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': False,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_coach = dotdict({\n",
    "    'numIters': 5,\n",
    "    'numEps': 25,\n",
    "    'tempThreshold': 15,\n",
    "    'updateThreshold': 0.6,\n",
    "    'maxlenOfQueue': 200000,\n",
    "    'numMCTSSims': 25,\n",
    "    'arenaCompare': 40,\n",
    "    'cpuct': 1,\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('./pretrained_models/tictactoe/keras','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 10,\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = TicTacToeGame(n=3)\n",
    "nnet = NNetWrapper(g, args_net)\n",
    "\n",
    "# path = '/home/marcin/Repos/alpha-zero-general/pretrained_models/tictactoe/keras/'\n",
    "# n1.load_checkpoint(path, 'best-25eps-25sim-10epch.pth.tar')\n",
    "\n",
    "c = Coach(g, nnet, args_coach, args_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ITER 1------\n",
      "Round completed 1/25\n",
      "Round completed 2/25\n",
      "Round completed 3/25\n",
      "Round completed 4/25\n",
      "Round completed 5/25\n",
      "Round completed 6/25\n",
      "Round completed 7/25\n",
      "Round completed 8/25\n",
      "Round completed 9/25\n",
      "Round completed 10/25\n",
      "Round completed 11/25\n",
      "Round completed 12/25\n",
      "Round completed 13/25\n",
      "Round completed 14/25\n",
      "Round completed 15/25\n",
      "Round completed 16/25\n",
      "Round completed 17/25\n",
      "Round completed 18/25\n",
      "Round completed 19/25\n",
      "Round completed 20/25\n",
      "Round completed 21/25\n",
      "Round completed 22/25\n",
      "Round completed 23/25\n",
      "Round completed 24/25\n",
      "Round completed 25/25\n",
      "Checkpoint Directory exists! \n",
      "WARNING:tensorflow:From /home/marcin/.anaconda/envs/keras113/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "1472/1472 [==============================] - 3s 2ms/step - loss: 3.4896 - pi_loss: 2.4659 - v_loss: 1.0236\n",
      "Epoch 2/10\n",
      "1472/1472 [==============================] - 1s 401us/step - loss: 3.0849 - pi_loss: 2.1817 - v_loss: 0.9032\n",
      "Epoch 3/10\n",
      "1472/1472 [==============================] - 1s 406us/step - loss: 2.9750 - pi_loss: 2.0825 - v_loss: 0.8926\n",
      "Epoch 4/10\n",
      "1472/1472 [==============================] - 1s 395us/step - loss: 2.9298 - pi_loss: 2.0139 - v_loss: 0.9160\n",
      "Epoch 5/10\n",
      "1472/1472 [==============================] - 1s 404us/step - loss: 2.6874 - pi_loss: 1.9571 - v_loss: 0.7303\n",
      "Epoch 6/10\n",
      "1472/1472 [==============================] - 1s 398us/step - loss: 2.6257 - pi_loss: 1.9174 - v_loss: 0.7083\n",
      "Epoch 7/10\n",
      "1472/1472 [==============================] - 1s 405us/step - loss: 2.5121 - pi_loss: 1.8504 - v_loss: 0.6617\n",
      "Epoch 8/10\n",
      "1472/1472 [==============================] - 1s 401us/step - loss: 2.4309 - pi_loss: 1.8158 - v_loss: 0.6150\n",
      "Epoch 9/10\n",
      "1472/1472 [==============================] - 1s 399us/step - loss: 2.4162 - pi_loss: 1.7979 - v_loss: 0.6183\n",
      "Epoch 10/10\n",
      "1472/1472 [==============================] - 1s 406us/step - loss: 2.3279 - pi_loss: 1.7598 - v_loss: 0.5681\n",
      "PITTING AGAINST PREVIOUS VERSION\n",
      "Game completed: (2/40)\n",
      "Game completed: (3/40)\n",
      "Game completed: (4/40)\n",
      "Game completed: (5/40)\n",
      "Game completed: (6/40)\n",
      "Game completed: (7/40)\n",
      "Game completed: (8/40)\n",
      "Game completed: (9/40)\n",
      "Game completed: (10/40)\n",
      "Game completed: (11/40)\n",
      "Game completed: (12/40)\n",
      "Game completed: (13/40)\n",
      "Game completed: (14/40)\n",
      "Game completed: (15/40)\n",
      "Game completed: (16/40)\n",
      "Game completed: (17/40)\n",
      "Game completed: (18/40)\n",
      "Game completed: (19/40)\n",
      "Game completed: (20/40)\n",
      "Game completed: (21/40)\n",
      "Game completed: (22/40)\n",
      "Game completed: (23/40)\n",
      "Game completed: (24/40)\n",
      "Game completed: (25/40)\n",
      "Game completed: (26/40)\n",
      "Game completed: (27/40)\n",
      "Game completed: (28/40)\n",
      "Game completed: (29/40)\n",
      "Game completed: (30/40)\n",
      "Game completed: (31/40)\n",
      "Game completed: (32/40)\n",
      "Game completed: (33/40)\n",
      "Game completed: (34/40)\n",
      "Game completed: (35/40)\n",
      "Game completed: (36/40)\n",
      "Game completed: (37/40)\n",
      "Game completed: (38/40)\n",
      "Game completed: (39/40)\n",
      "Game completed: (40/40)\n",
      "Game completed: (41/40)\n",
      "NEW/PREV WINS : 18 / 2 ; DRAWS : 20\n",
      "ACCEPTING NEW MODEL\n",
      "Checkpoint Directory exists! \n",
      "Checkpoint Directory exists! \n",
      "------ITER 2------\n",
      "Round completed 1/25\n",
      "Round completed 2/25\n",
      "Round completed 3/25\n",
      "Round completed 4/25\n",
      "Round completed 5/25\n",
      "Round completed 6/25\n",
      "Round completed 7/25\n",
      "Round completed 8/25\n",
      "Round completed 9/25\n",
      "Round completed 10/25\n",
      "Round completed 11/25\n",
      "Round completed 12/25\n",
      "Round completed 13/25\n",
      "Round completed 14/25\n",
      "Round completed 15/25\n",
      "Round completed 16/25\n",
      "Round completed 17/25\n",
      "Round completed 18/25\n",
      "Round completed 19/25\n",
      "Round completed 20/25\n",
      "Round completed 21/25\n",
      "Round completed 22/25\n",
      "Round completed 23/25\n",
      "Round completed 24/25\n",
      "Round completed 25/25\n",
      "Checkpoint Directory exists! \n",
      "Epoch 1/10\n",
      "2936/2936 [==============================] - 1s 457us/step - loss: 2.1680 - pi_loss: 1.6024 - v_loss: 0.5656\n",
      "Epoch 2/10\n",
      "2936/2936 [==============================] - 1s 405us/step - loss: 2.0479 - pi_loss: 1.5485 - v_loss: 0.4994\n",
      "Epoch 3/10\n",
      "2936/2936 [==============================] - 1s 412us/step - loss: 1.9683 - pi_loss: 1.5067 - v_loss: 0.4616\n",
      "Epoch 4/10\n",
      "2936/2936 [==============================] - 1s 402us/step - loss: 1.9091 - pi_loss: 1.4748 - v_loss: 0.4342\n",
      "Epoch 5/10\n",
      "2936/2936 [==============================] - 1s 402us/step - loss: 1.9009 - pi_loss: 1.4728 - v_loss: 0.4281\n",
      "Epoch 6/10\n",
      "2936/2936 [==============================] - 1s 407us/step - loss: 1.8808 - pi_loss: 1.4559 - v_loss: 0.4249\n",
      "Epoch 7/10\n",
      "2936/2936 [==============================] - 1s 404us/step - loss: 1.8274 - pi_loss: 1.4307 - v_loss: 0.3966\n",
      "Epoch 8/10\n",
      "2936/2936 [==============================] - 1s 404us/step - loss: 1.8074 - pi_loss: 1.4240 - v_loss: 0.3834\n",
      "Epoch 9/10\n",
      "2936/2936 [==============================] - 1s 402us/step - loss: 1.8061 - pi_loss: 1.4194 - v_loss: 0.3867\n",
      "Epoch 10/10\n",
      "2936/2936 [==============================] - 1s 403us/step - loss: 1.7809 - pi_loss: 1.4079 - v_loss: 0.3730\n",
      "PITTING AGAINST PREVIOUS VERSION\n",
      "Game completed: (2/40)\n",
      "Game completed: (3/40)\n",
      "Game completed: (4/40)\n",
      "Game completed: (5/40)\n",
      "Game completed: (6/40)\n",
      "Game completed: (7/40)\n",
      "Game completed: (8/40)\n",
      "Game completed: (9/40)\n",
      "Game completed: (10/40)\n",
      "Game completed: (11/40)\n",
      "Game completed: (12/40)\n",
      "Game completed: (13/40)\n",
      "Game completed: (14/40)\n",
      "Game completed: (15/40)\n",
      "Game completed: (16/40)\n",
      "Game completed: (17/40)\n",
      "Game completed: (18/40)\n",
      "Game completed: (19/40)\n",
      "Game completed: (20/40)\n",
      "Game completed: (21/40)\n",
      "Game completed: (22/40)\n",
      "Game completed: (23/40)\n",
      "Game completed: (24/40)\n",
      "Game completed: (25/40)\n",
      "Game completed: (26/40)\n",
      "Game completed: (27/40)\n",
      "Game completed: (28/40)\n",
      "Game completed: (29/40)\n",
      "Game completed: (30/40)\n",
      "Game completed: (31/40)\n",
      "Game completed: (32/40)\n",
      "Game completed: (33/40)\n",
      "Game completed: (34/40)\n",
      "Game completed: (35/40)\n",
      "Game completed: (36/40)\n",
      "Game completed: (37/40)\n",
      "Game completed: (38/40)\n",
      "Game completed: (39/40)\n",
      "Game completed: (40/40)\n",
      "Game completed: (41/40)\n",
      "NEW/PREV WINS : 1 / 0 ; DRAWS : 39\n",
      "ACCEPTING NEW MODEL\n",
      "Checkpoint Directory exists! \n",
      "Checkpoint Directory exists! \n",
      "------ITER 3------\n",
      "Round completed 1/25\n",
      "Round completed 2/25\n",
      "Round completed 3/25\n",
      "Round completed 4/25\n",
      "Round completed 5/25\n",
      "Round completed 6/25\n",
      "Round completed 7/25\n",
      "Round completed 8/25\n",
      "Round completed 9/25\n",
      "Round completed 10/25\n",
      "Round completed 11/25\n",
      "Round completed 12/25\n",
      "Round completed 13/25\n",
      "Round completed 14/25\n",
      "Round completed 15/25\n",
      "Round completed 16/25\n",
      "Round completed 17/25\n",
      "Round completed 18/25\n",
      "Round completed 19/25\n",
      "Round completed 20/25\n",
      "Round completed 21/25\n",
      "Round completed 22/25\n",
      "Round completed 23/25\n",
      "Round completed 24/25\n",
      "Round completed 25/25\n",
      "Checkpoint Directory exists! \n",
      "Epoch 1/10\n",
      "4560/4560 [==============================] - 2s 429us/step - loss: 1.5914 - pi_loss: 1.2455 - v_loss: 0.3459\n",
      "Epoch 2/10\n",
      "4560/4560 [==============================] - 2s 408us/step - loss: 1.5675 - pi_loss: 1.2304 - v_loss: 0.3370\n",
      "Epoch 3/10\n",
      "4560/4560 [==============================] - 2s 407us/step - loss: 1.5573 - pi_loss: 1.2258 - v_loss: 0.3316\n",
      "Epoch 4/10\n",
      "4560/4560 [==============================] - 2s 410us/step - loss: 1.5438 - pi_loss: 1.2151 - v_loss: 0.3287\n",
      "Epoch 5/10\n",
      "4560/4560 [==============================] - 2s 406us/step - loss: 1.5268 - pi_loss: 1.2062 - v_loss: 0.3207\n",
      "Epoch 6/10\n",
      "4560/4560 [==============================] - 2s 407us/step - loss: 1.5205 - pi_loss: 1.2051 - v_loss: 0.3154\n",
      "Epoch 7/10\n",
      "4560/4560 [==============================] - 2s 407us/step - loss: 1.5214 - pi_loss: 1.2046 - v_loss: 0.3168\n",
      "Epoch 8/10\n",
      "4560/4560 [==============================] - 2s 407us/step - loss: 1.5202 - pi_loss: 1.2042 - v_loss: 0.3160\n",
      "Epoch 9/10\n",
      "4560/4560 [==============================] - 2s 404us/step - loss: 1.5078 - pi_loss: 1.1916 - v_loss: 0.3162\n",
      "Epoch 10/10\n",
      "4560/4560 [==============================] - 2s 410us/step - loss: 1.5041 - pi_loss: 1.1912 - v_loss: 0.3129\n",
      "PITTING AGAINST PREVIOUS VERSION\n",
      "Game completed: (2/40)\n",
      "Game completed: (3/40)\n",
      "Game completed: (4/40)\n",
      "Game completed: (5/40)\n",
      "Game completed: (6/40)\n",
      "Game completed: (7/40)\n",
      "Game completed: (8/40)\n",
      "Game completed: (9/40)\n",
      "Game completed: (10/40)\n",
      "Game completed: (11/40)\n",
      "Game completed: (12/40)\n",
      "Game completed: (13/40)\n",
      "Game completed: (14/40)\n",
      "Game completed: (15/40)\n",
      "Game completed: (16/40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game completed: (17/40)\n",
      "Game completed: (18/40)\n",
      "Game completed: (19/40)\n",
      "Game completed: (20/40)\n",
      "Game completed: (21/40)\n",
      "Game completed: (22/40)\n",
      "Game completed: (23/40)\n",
      "Game completed: (24/40)\n",
      "Game completed: (25/40)\n",
      "Game completed: (26/40)\n",
      "Game completed: (27/40)\n",
      "Game completed: (28/40)\n",
      "Game completed: (29/40)\n",
      "Game completed: (30/40)\n",
      "Game completed: (31/40)\n",
      "Game completed: (32/40)\n",
      "Game completed: (33/40)\n",
      "Game completed: (34/40)\n",
      "Game completed: (35/40)\n",
      "Game completed: (36/40)\n",
      "Game completed: (37/40)\n",
      "Game completed: (38/40)\n",
      "Game completed: (39/40)\n",
      "Game completed: (40/40)\n",
      "Game completed: (41/40)\n",
      "NEW/PREV WINS : 0 / 0 ; DRAWS : 40\n",
      "REJECTING NEW MODEL\n",
      "------ITER 4------\n",
      "Round completed 1/25\n",
      "Round completed 2/25\n",
      "Round completed 3/25\n",
      "Round completed 4/25\n",
      "Round completed 5/25\n",
      "Round completed 6/25\n",
      "Round completed 7/25\n",
      "Round completed 8/25\n",
      "Round completed 9/25\n",
      "Round completed 10/25\n",
      "Round completed 11/25\n",
      "Round completed 12/25\n",
      "Round completed 13/25\n",
      "Round completed 14/25\n",
      "Round completed 15/25\n",
      "Round completed 16/25\n",
      "Round completed 17/25\n",
      "Round completed 18/25\n",
      "Round completed 19/25\n",
      "Round completed 20/25\n",
      "Round completed 21/25\n",
      "Round completed 22/25\n",
      "Round completed 23/25\n",
      "Round completed 24/25\n",
      "Round completed 25/25\n",
      "Checkpoint Directory exists! \n",
      "Epoch 1/10\n",
      "6304/6304 [==============================] - 3s 422us/step - loss: 1.4573 - pi_loss: 1.1611 - v_loss: 0.2962\n",
      "Epoch 2/10\n",
      "6304/6304 [==============================] - 3s 403us/step - loss: 1.4100 - pi_loss: 1.1294 - v_loss: 0.2805\n",
      "Epoch 3/10\n",
      "6304/6304 [==============================] - 3s 404us/step - loss: 1.3766 - pi_loss: 1.1081 - v_loss: 0.2685\n",
      "Epoch 4/10\n",
      "6304/6304 [==============================] - 3s 406us/step - loss: 1.3576 - pi_loss: 1.0966 - v_loss: 0.2611\n",
      "Epoch 5/10\n",
      "6304/6304 [==============================] - 3s 403us/step - loss: 1.3411 - pi_loss: 1.0840 - v_loss: 0.2572\n",
      "Epoch 6/10\n",
      "6304/6304 [==============================] - 3s 406us/step - loss: 1.3338 - pi_loss: 1.0781 - v_loss: 0.2557\n",
      "Epoch 7/10\n",
      "6304/6304 [==============================] - 3s 405us/step - loss: 1.3219 - pi_loss: 1.0731 - v_loss: 0.2488\n",
      "Epoch 8/10\n",
      "6304/6304 [==============================] - 3s 406us/step - loss: 1.3172 - pi_loss: 1.0692 - v_loss: 0.2480\n",
      "Epoch 9/10\n",
      "6304/6304 [==============================] - 3s 405us/step - loss: 1.3073 - pi_loss: 1.0664 - v_loss: 0.2409\n",
      "Epoch 10/10\n",
      "6304/6304 [==============================] - 3s 406us/step - loss: 1.3082 - pi_loss: 1.0631 - v_loss: 0.2451\n",
      "PITTING AGAINST PREVIOUS VERSION\n",
      "Game completed: (2/40)\n",
      "Game completed: (3/40)\n",
      "Game completed: (4/40)\n",
      "Game completed: (5/40)\n",
      "Game completed: (6/40)\n",
      "Game completed: (7/40)\n",
      "Game completed: (8/40)\n",
      "Game completed: (9/40)\n",
      "Game completed: (10/40)\n",
      "Game completed: (11/40)\n",
      "Game completed: (12/40)\n",
      "Game completed: (13/40)\n",
      "Game completed: (14/40)\n",
      "Game completed: (15/40)\n",
      "Game completed: (16/40)\n",
      "Game completed: (17/40)\n",
      "Game completed: (18/40)\n",
      "Game completed: (19/40)\n",
      "Game completed: (20/40)\n",
      "Game completed: (21/40)\n",
      "Game completed: (22/40)\n",
      "Game completed: (23/40)\n",
      "Game completed: (24/40)\n",
      "Game completed: (25/40)\n",
      "Game completed: (26/40)\n",
      "Game completed: (27/40)\n",
      "Game completed: (28/40)\n",
      "Game completed: (29/40)\n",
      "Game completed: (30/40)\n",
      "Game completed: (31/40)\n",
      "Game completed: (32/40)\n",
      "Game completed: (33/40)\n",
      "Game completed: (34/40)\n",
      "Game completed: (35/40)\n",
      "Game completed: (36/40)\n",
      "Game completed: (37/40)\n",
      "Game completed: (38/40)\n",
      "Game completed: (39/40)\n",
      "Game completed: (40/40)\n",
      "Game completed: (41/40)\n",
      "NEW/PREV WINS : 0 / 0 ; DRAWS : 40\n",
      "REJECTING NEW MODEL\n",
      "------ITER 5------\n",
      "Round completed 1/25\n",
      "Round completed 2/25\n",
      "Round completed 3/25\n",
      "Round completed 4/25\n",
      "Round completed 5/25\n",
      "Round completed 6/25\n",
      "Round completed 7/25\n",
      "Round completed 8/25\n",
      "Round completed 9/25\n",
      "Round completed 10/25\n",
      "Round completed 11/25\n",
      "Round completed 12/25\n",
      "Round completed 13/25\n",
      "Round completed 14/25\n",
      "Round completed 15/25\n",
      "Round completed 16/25\n",
      "Round completed 17/25\n",
      "Round completed 18/25\n",
      "Round completed 19/25\n",
      "Round completed 20/25\n",
      "Round completed 21/25\n",
      "Round completed 22/25\n",
      "Round completed 23/25\n",
      "Round completed 24/25\n",
      "Round completed 25/25\n",
      "Checkpoint Directory exists! \n",
      "Epoch 1/10\n",
      "7944/7944 [==============================] - 3s 423us/step - loss: 1.4259 - pi_loss: 1.1454 - v_loss: 0.2805\n",
      "Epoch 2/10\n",
      "7944/7944 [==============================] - 3s 417us/step - loss: 1.3219 - pi_loss: 1.0734 - v_loss: 0.2485\n",
      "Epoch 3/10\n",
      "7944/7944 [==============================] - 3s 409us/step - loss: 1.2840 - pi_loss: 1.0462 - v_loss: 0.2379\n",
      "Epoch 4/10\n",
      "7944/7944 [==============================] - 3s 408us/step - loss: 1.2771 - pi_loss: 1.0428 - v_loss: 0.2343\n",
      "Epoch 5/10\n",
      "7944/7944 [==============================] - 3s 406us/step - loss: 1.2510 - pi_loss: 1.0255 - v_loss: 0.2255\n",
      "Epoch 6/10\n",
      "7944/7944 [==============================] - 3s 410us/step - loss: 1.2406 - pi_loss: 1.0161 - v_loss: 0.2246\n",
      "Epoch 7/10\n",
      "7944/7944 [==============================] - 3s 408us/step - loss: 1.2297 - pi_loss: 1.0093 - v_loss: 0.2203\n",
      "Epoch 8/10\n",
      "7944/7944 [==============================] - 3s 410us/step - loss: 1.2246 - pi_loss: 1.0071 - v_loss: 0.2175\n",
      "Epoch 9/10\n",
      "7944/7944 [==============================] - 3s 411us/step - loss: 1.2198 - pi_loss: 1.0033 - v_loss: 0.2165\n",
      "Epoch 10/10\n",
      "7944/7944 [==============================] - 3s 409us/step - loss: 1.2300 - pi_loss: 1.0118 - v_loss: 0.2182\n",
      "PITTING AGAINST PREVIOUS VERSION\n",
      "Game completed: (2/40)\n",
      "Game completed: (3/40)\n",
      "Game completed: (4/40)\n",
      "Game completed: (5/40)\n",
      "Game completed: (6/40)\n",
      "Game completed: (7/40)\n",
      "Game completed: (8/40)\n",
      "Game completed: (9/40)\n",
      "Game completed: (10/40)\n",
      "Game completed: (11/40)\n",
      "Game completed: (12/40)\n",
      "Game completed: (13/40)\n",
      "Game completed: (14/40)\n",
      "Game completed: (15/40)\n",
      "Game completed: (16/40)\n",
      "Game completed: (17/40)\n",
      "Game completed: (18/40)\n",
      "Game completed: (19/40)\n",
      "Game completed: (20/40)\n",
      "Game completed: (21/40)\n",
      "Game completed: (22/40)\n",
      "Game completed: (23/40)\n",
      "Game completed: (24/40)\n",
      "Game completed: (25/40)\n",
      "Game completed: (26/40)\n",
      "Game completed: (27/40)\n",
      "Game completed: (28/40)\n",
      "Game completed: (29/40)\n",
      "Game completed: (30/40)\n",
      "Game completed: (31/40)\n",
      "Game completed: (32/40)\n",
      "Game completed: (33/40)\n",
      "Game completed: (34/40)\n",
      "Game completed: (35/40)\n",
      "Game completed: (36/40)\n",
      "Game completed: (37/40)\n",
      "Game completed: (38/40)\n",
      "Game completed: (39/40)\n",
      "Game completed: (40/40)\n",
      "Game completed: (41/40)\n",
      "NEW/PREV WINS : 0 / 0 ; DRAWS : 40\n",
      "REJECTING NEW MODEL\n"
     ]
    }
   ],
   "source": [
    "c.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = TicTacToeGame(n=3)\n",
    "\n",
    "rp = RandomPlayer(g).play\n",
    "hp = HumanTicTacToePlayer(g).play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': False,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = NNetWrapper(g, args)\n",
    "# path = '/home/marcin/Repos/alpha-zero-general/pretrained_models/tictactoe/keras/'\n",
    "path = 'temp/'\n",
    "n1.load_checkpoint(path, 'best.pth.tar')\n",
    "args1 = dotdict({'numMCTSSims': 25, 'cpuct':1.0})\n",
    "mcts1 = MCTS(g, n1, args1)\n",
    "n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena = Arena(n1p, hp, g, display=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn  1 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |- - - |\n",
      "1 |- - - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "Turn  2 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |- - - |\n",
      "1 |- O - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "0 0\n",
      "Turn  3 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |- O - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "Turn  4 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O - |\n",
      "1 |- O - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 1\n",
      "Turn  5 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O - |\n",
      "1 |- O - |\n",
      "2 |- X - |\n",
      "  --------\n",
      "Turn  6 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O - |\n",
      "1 |- O - |\n",
      "2 |O X - |\n",
      "  --------\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 2\n",
      "0 2\n",
      "Turn  7 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O X |\n",
      "1 |- O - |\n",
      "2 |O X - |\n",
      "  --------\n",
      "Turn  8 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O X |\n",
      "1 |- O - |\n",
      "2 |O X O |\n",
      "  --------\n",
      "1 0\n",
      "1 2\n",
      "1 2\n",
      "Turn  9 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O X |\n",
      "1 |- O X |\n",
      "2 |O X O |\n",
      "  --------\n",
      "Game over: Turn  9 Result  0.0001\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O X |\n",
      "1 |O O X |\n",
      "2 |O X O |\n",
      "  --------\n",
      "Game completed: (2/2)\n",
      "Turn  1 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |- - - |\n",
      "1 |- - - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "1 1\n",
      "Turn  2 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |- - - |\n",
      "1 |- O - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "Turn  3 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |- O - |\n",
      "2 |- - - |\n",
      "  --------\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 2\n",
      "Turn  4 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |- O - |\n",
      "2 |- - O |\n",
      "  --------\n",
      "Turn  5 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |- O - |\n",
      "2 |X - O |\n",
      "  --------\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 2\n",
      "2 1\n",
      "1 0\n",
      "Turn  6 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |O O - |\n",
      "2 |X - O |\n",
      "  --------\n",
      "Turn  7 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - - |\n",
      "1 |O O X |\n",
      "2 |X - O |\n",
      "  --------\n",
      "0 1\n",
      "0 2\n",
      "2 1\n",
      "0 2\n",
      "Turn  8 Player  -1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - O |\n",
      "1 |O O X |\n",
      "2 |X - O |\n",
      "  --------\n",
      "Turn  9 Player  1\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X - O |\n",
      "1 |O O X |\n",
      "2 |X X O |\n",
      "  --------\n",
      "0 1\n",
      "0 1\n",
      "Game over: Turn  9 Result  0.0001\n",
      "   0 1 2 \n",
      "  --------\n",
      "0 |X O O |\n",
      "1 |O O X |\n",
      "2 |X X O |\n",
      "  --------\n",
      "Game completed: (3/2)\n",
      "(0, 0, 2)\n"
     ]
    }
   ],
   "source": [
    "print(arena.playGames(2, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras113]",
   "language": "python",
   "name": "conda-env-keras113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
